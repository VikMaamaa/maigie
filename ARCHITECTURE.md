# Maigie Architecture & System Flows (Expanded)

## Overview

Maigie is an AI-powered student companion that helps learners manage courses, set goals, discover resources, schedule study sessions, get forecasts and reminders, and converse with an intelligent assistant (text + voice). The codebase is a monorepo managed with **Nx**. The frontend stack uses **Vite + shadcn-ui** for web and **Expo** for mobile (React Native). The backend is **FastAPI** with clear service separation and AI integrations (vector DB, embeddings, LLMs).

---

# 1. Monorepo (Nx) Layout — practical details

```
Maigie/
  ├─ apps/
  │   ├─ backend/                # FastAPI app (Python)
  │   ├─ web/                    # Vite + React (shadcn-ui)
  │   └─ mobile/                 # Expo (React Native)
  ├─ libs/
  │   ├─ types/                  # shared TypeScript types & API client
  │   ├─ ui/                     # shared UI components (web & mobile where possible)
  │   ├─ auth/                   # shared auth helpers (token helpers)
  │   ├─ ai/                     # shared prompts, schema for AI interactions
  │   └─ db/                     # Prisma schema + migrations (or SQLModel models)
  ├─ tools/
  ├─ nx.json
  ├─ package.json
  ├─ pyproject.toml
  └─ README.md
```

Notes:

* Keep a small, well-documented `types` lib so web & mobile share DTOs and reduce drift.
* Use Nx tasks to run lint/test/build for affected projects.

---

# 2. High-level Requirements & Features

* **AI‑first operations**: Courses, goals, and schedules can be *auto-generated* by the AI assistant during conversations.

  * User can manually edit or add more items, but AI generation is the default.
  * Example: User says *“I want to learn Data Structures for my exam in 6 weeks”* → AI creates a course outline, sets goals, and builds a study schedule.
  * Example: User says *“Help me plan tomorrow”* → AI analyzes workload and generates a daily schedule.

* User auth (email/password, social sign-in)

* Create & manage Courses (AI-generated syllabus or manual input)

* Create Goals (AI interprets intent and sets milestones automatically)

* Resource recommendations (links, videos, PDFs) — generated by AI and curated by user

* Schedule (daily view, time blocks, recurring sessions) — AI suggests optimized plan automatically

* Forecast (AI-driven estimates of progress by date given study habits)

* Reminders & notifications (push, email)

* AI Assistant (chat + voice): ask about courses, get study plans, request resources, ask clarifying questions

* Dashboard organization: courses, goals, resources, daily schedule, forecast, reminders

* Offline-first support on mobile (Expo + local DB like WatermelonDB / SQLite sync)

* Analytics, usage metrics, privacy and consent controls

* User auth (email/password, social sign-in)

* Create & manage Courses (title, modules, syllabus)

* Create Goals (with target dates, progress tracking)

* Resource recommendations (links, videos, PDFs) — generated by AI and curated by user

* Schedule (daily view, time blocks, recurring sessions)

* Forecast (AI-driven estimates of progress by date given study habits)

* Reminders & notifications (push, email)

* AI Assistant (chat + voice): ask about courses, get study plans, request resources, ask clarifying questions

* Dashboard organization: courses, goals, resources, daily schedule, forecast, reminders

* Offline-first support on mobile (Expo + local DB like WatermelonDB / SQLite sync)

* Analytics, usage metrics, privacy and consent controls

---

# 3. Backend (FastAPI) — Project structure & specs

```
backend/
  ├─ src/
  │   ├─ main.py                # FastAPI app factory
  │   ├─ routes/
  │   │   ├─ auth.py
  │   │   ├─ users.py
  │   │   ├─ courses.py
  │   │   ├─ goals.py
  │   │   ├─ resources.py
  │   │   ├─ schedule.py
  │   │   ├─ ai.py
  │   │   └─ realtime.py         # websocket endpoints
  │   ├─ services/              # business logic
  │   ├─ models/                # Pydantic schemas + ORM models
  │   ├─ db/                    # DB connection, migrations
  │   ├─ tasks/                 # background tasks (Celery/Dramatiq)
  │   ├─ ai_client/             # LLM and embeddings clients, prompt templates
  │   ├─ workers/               # async workers for indexing, recommendations
  │   └─ utils/
  └─ tests/
```

### API design principles

* REST for resources + a few RPC-like endpoints for AI actions. Use consistent versioning: `/api/v1/...`.
* All requests validated with Pydantic.
* Paginate list endpoints, support filters and full-text/semantic search.
* Rate-limit AI endpoints per-user.

---

# 4. Core API Endpoints (summary)

### Auth

* `POST /api/v1/auth/register` — body: `{ email, password, name? }`
* `POST /api/v1/auth/login` — returns `access_token` (JWT), `refresh_token`
* `POST /api/v1/auth/refresh`
* `GET /api/v1/auth/me`

### Users

* `GET /api/v1/users/{id}`
* `PATCH /api/v1/users/{id}`

### Courses

* `GET /api/v1/courses` — list (filters: enrolled, created)
* `POST /api/v1/courses` — create course
* `GET /api/v1/courses/{id}` — details (modules)
* `POST /api/v1/courses/{id}/enroll`

### Goals

* `GET /api/v1/goals`
* `POST /api/v1/goals`
* `PATCH /api/v1/goals/{id}`
* `POST /api/v1/goals/{id}/progress` — record progress event

### Resources

* `GET /api/v1/resources` — (search, filter by type)
* `POST /api/v1/resources` — add manual resource or add AI-recommended resource
* `POST /api/v1/resources/recommend` — body: `{ context: {courses, goals, recentActivity} }` → returns recommended resources (uses embeddings + LLM)

### Schedule & Reminders

* `GET /api/v1/schedule?date=YYYY-MM-DD`
* `POST /api/v1/schedule` — create time block (title, start, end, recurring)
* `POST /api/v1/reminders` — schedule notification

### AI Assistant

* `POST /api/v1/ai/chat` — body: `{ message, contextRefs?: [noteIds, courseIds], voice?: false }` → returns assistant message
* `POST /api/v1/ai/voice-session` — start voice channel (websocket or WebRTC signaling)
* `POST /api/v1/ai/summary` — summarize a note or course
* `POST /api/v1/ai/create-plan` — `{ goalId | courseId }` → returns study plan with schedule suggestions

### Realtime

* `GET /api/v1/realtime/ws` — websocket for chat + live progress updates

---

# 5. Data Models (core tables) — simplified ERD

Entities (high-level): Users, Courses, Modules, Enrollments, Goals, Tasks, Notes, Resources, Schedules, Reminders, AIConversations, Embeddings

Example schemas (Postgres / Prisma-like):

```
User(id PK, email, name, password_hash, created_at, timezone, preferences JSON)
Course(id PK, ownerId FK->User, title, description, metadata JSON, created_at)
Module(id PK, courseId FK->Course, title, content, order)
Enrollment(id PK, userId FK->User, courseId FK->Course, role, enrolled_at)
Goal(id PK, userId, title, description, target_date, status, progress float)
Task(id PK, userId, goalId?, courseId?, title, notes, due_at, status)
Note(id PK, userId, content, title, linkedCourseId?, embeddings_id)
Resource(id PK, title, url, type(enum), metadata JSON)
ScheduleBlock(id PK, userId, title, start_ts, end_ts, recurring_rule)
AIConversation(id PK, userId, messages JSON, metadata)
Embedding(id PK, object_type, object_id, vector)
```

Notes:

* Store `preferences` and `metadata` as JSONB for flexibility.
* Use `pgvector` or an external vector DB (Qdrant/Pinecone). If using Postgres+pgvector, keep vectors in a separate table to avoid bloat.

---

# 6. AI & Vector Search Design

### Ingestion

* On note/course/resource create or update → enqueue indexing job (background worker) to compute embeddings and store in vector DB.

### Semantic search

* Use embeddings to query relevant notes/resources.
* Combine with lightweight retrieval-augmented generation (RAG) for chat: get top-k docs → build prompt → call LLM.

### Prompting

* Keep prompt templates in `libs/ai/` and version them.
* Always include a short context (user preferences, active course, recent activity) and a retrieval buffer.

### Rate limits & quotas

* Per-user and global limits; cache LLM responses for identical queries.

---

# 7. Realtime & Voice

### Realtime chat

* Use WebSockets (FastAPI `websockets`) or a managed service (Pusher, Ably) for real-time messaging and progress updates.
* Messages are persisted to `AIConversation` and optionally indexed.

### Voice (Realtime conversation)

Options:

1. WebRTC for low-latency audio streams — signaling handled by FastAPI WebSocket endpoint, media servers as needed.
2. For simpler implementation: use client-side recording → send audio chunks to `/api/v1/ai/voice` (multipart) → backend streams to STT (Speech-to-Text) → process text with LLM → TTS response streamed back.

Suggested flow for MVP voice:

* Client records short chunks (1–5s) and streams to `/ai/voice-stream` via websocket.
* Backend forwards to STT (e.g., OpenAI/whisper, cloud STT) and receives partial transcripts.
* Backend sends transcript to AI chat pipeline and returns assistant response text.
* TTS (e.g., gTTS, cloud TTS) produces audio and server streams playable URL or base64 audio chunk back to client.

Security: require auth token for voice endpoints; limit duration & rate.

---

# 8. Dashboard & Frontend Organization (detailed)

Dashboard components (web + mobile):

* Header: quick search, user menu, notifications
* Left nav: Courses, Goals, Resources, Schedule, AI Assistant
* Center: Today view (schedule blocks), Active Goal(s), Quick actions (add note, start study session)
* Right / widgets: Forecast (AI estimate), Reminders, Recent resources

Data displayed:

* Courses: progress percentage, next session, recent notes
* Goals: progress bars with milestone markers
* Resources: starred, recommended (AI), recent
* Schedules: today timeline with collapsing time blocks
* Forecast: "If you study X hours/day you will reach Y% by DATE"

Interaction patterns:

* Click a course → course detail page (modules, notes, recommended resources)
* Chat assistant modal/panel → context-aware (auto-select active course or goal)
* Quick-add from dashboard: create note, schedule block, or goal

UI tech notes:

* Web: Vite app with shadcn-ui components, Tailwind CSS configuration
* Mobile: Expo + React Navigation; reuse UI tokens via `libs/ui` and small platform-specific components
* Shared API client auto-generated from OpenAPI schema (fastapi can provide `/openapi.json`) and kept in `libs/types`.

---

# 9. Offline-first & Sync (Mobile)

* Local store: Expo SQLite (with WatermelonDB or direct sqlite) to persist notes and schedule.
* Sync strategy: optimistic writes, background sync when online, conflict resolution by timestamp and last-writer-wins or user merge UI for complex conflicts.
* Queue outbound actions (create note, schedule) to be retried until acknowledged.

---

# 10. Background Processing & Jobs

Use Celery or Dramatiq with Redis as broker (or serverless jobs):

* Embedding indexing
* Email & push notifications
* Async resource scraping (fetch metadata for resource URLs)
* Daily digest & forecast recalculation

---

# 11. Deployment & Infra

* Backend: containerized (Docker), deploy to Render/Fly.io/AWS ECS/GCP Cloud Run
* DB: Postgres (Neon / Supabase / RDS)
* Vector DB: Qdrant / Pinecone or Postgres+pgvector
* Redis: Upstash or managed Redis
* Storage: S3-compatible (Supabase Storage / AWS S3)
* Secrets: Vault / cloud secret manager

CI/CD:

* Nx-aware pipelines to test, build, and deploy only affected apps/libraries
* Unit & integration tests (pytest), e2e tests for web (Playwright) and mobile (Detox / Expo E2E)

---

# 12. Observability & SLOs

* Logging: structured logs (JSON) -> centralized (Datadog / Loki)
* Tracing: OpenTelemetry for request traces (especially AI calls)
* Metrics: Prometheus / Grafana for API latency, error rate, queue lengths
* SLO example: 99% API availability, LLM latency under X ms (where feasible)

---

# 13. Security & Privacy

* Encrypt sensitive fields at rest (where needed).
* GDPR-style controls: data export, delete account, explicit consent for storage of audio and AI logs.
* Use rate limiting, input sanitization, and prompt injection mitigations (sanitize retrieved docs, QA-checks on prompts).
* Use short-lived JWT access tokens + refresh tokens; store refresh tokens securely in DB

---

# 14. API Specification / OpenAPI

* FastAPI will auto-generate an OpenAPI spec; publish and generate TypeScript client for frontend.
* Use strict schema validation for all endpoints and return standardized error object `{ code, message, details? }`.

---

# 15. Example Flows (detailed)

## 15.1: "Chat about what courses I'm offering"

1. User opens AI Assistant (context includes: user owned courses & selected course)
2. Client sends `{ message: "Summarize the courses I'm offering", context: { scope: "ownedCourses" } }` to `/api/v1/ai/chat`
3. Backend retrieves user's course list, composes a retrieval buffer, and calls LLM.
4. LLM returns structured reply: summary + suggestions (e.g. gaps in syllabus)
5. Backend persists conversation and returns response. Optionally UI shows quick actions (edit course, add module)

## 15.2: "Set goal and get recommended resources"

1. User creates goal via `POST /goals`
2. Backend creates Goal and enqueues `recommendation` job.
3. Worker collects course context, notes, and queries vector DB for resources, then calls LLM to synthesize a prioritized list.
4. Recommendations saved to `resources` with `recommendation_source=ai` and surfaced on dashboard.

## 15.3: "Daily Forecast"

1. Daily cron runs per user: gather historical study session durations, task completion rates.
2. AI worker predicts completion likelihood for active goals and returns a calendar forecast for the next 7/14/30 days.
3. Dashboard shows forecast widget with recommended study time per day to meet target date.

---

# 16. Non-functional considerations

* Design for incremental adoption: start with text-chat, notes, courses and a simple scheduler; add voice and forecasting later.
* Keep per-user data partitioned and scalable.
* Cost control for LLM usage: caching, summarization instead of full re-generation for repeated queries, tiered quotas.

---

# 17. AI Intent → Action Mapping Table

Below is the system‑level mapping of what the AI should do for each type of user intent. These mappings are used by the backend (FastAPI) AI service to interpret conversation and trigger structured actions.

---

## 1. Learning / Course Creation Intents

| User Intent (Natural)                      | AI Interpretation        | Backend Actions                                                                                                |
| ------------------------------------------ | ------------------------ | -------------------------------------------------------------------------------------------------------------- |
| "I want to learn X"                        | Create a course outline  | → LLM generates syllabus → Create `Course` + `Modules` → Index for search → Return summary to UI               |
| "Teach me about Y"                         | Course/topic exploration | → Retrieve relevant notes/resources → Summaries → Suggestions to create a course if structured learning needed |
| "I’m starting a new class on Z"            | Add external course      | → Create `Course` with minimal metadata → Ask user to upload syllabus or extract from text                     |
| "Explain what topics I need for this exam" | Derive learning pathway  | → AI generates outline → Offer auto‑create course + set schedule                                               |

---

## 2. Goal Setting Intents

| User Intent                                  | AI Interpretation                  | Backend Actions                                                         |
| -------------------------------------------- | ---------------------------------- | ----------------------------------------------------------------------- |
| "I want to pass my exam in 6 weeks"          | Create academic goal with deadline | → Create `Goal` → Estimate workload → Attach relevant courses           |
| "Help me improve in math this month"         | High-level skill improvement goal  | → Generate milestones → Create `Goal` with weekly checkpoints           |
| "I want to finish Chapter 5 today"           | Short-term goal                    | → Create micro-goal → Add to today's schedule                           |
| "What should my goals be for this semester?" | Goal planning                      | → Use existing courses → Generate 3–5 recommended goals → User approves |

---

## 3. Scheduling Intents

| User Intent                            | AI Interpretation          | Backend Actions                                                                     |
| -------------------------------------- | -------------------------- | ----------------------------------------------------------------------------------- |
| "Plan my week"                         | Weekly schedule generation | → Fetch courses, goals → Create recurring schedule blocks → Save to `ScheduleBlock` |
| "Help me study today"                  | Daily study plan           | → Review time availability → Generate optimized timetable → Add reminders           |
| "Give me a timetable for this month"   | Long-range schedule        | → Create 4-week plan → Group by focus areas → Save to schedule + forecast           |
| "Schedule 2 hours of reading tomorrow" | Specific time block        | → Create one-off schedule block                                                     |

---

## 4. Resource Discovery Intents

| User Intent                                    | AI Interpretation          | Backend Actions                                                   |
| ---------------------------------------------- | -------------------------- | ----------------------------------------------------------------- |
| "Give me resources on X"                       | Resource recommendation    | → Query semantic search → LLM ranking → Create `Resource` entries |
| "What should I watch/read to understand this?" | Guided learning            | → Course/topic detection → Recommend videos/articles              |
| "Find me practice questions"                   | Assessment resource search | → Retrieve curated practice sets → Recommend based on difficulty  |
| "Show me more like this"                       | Similar resource query     | → Vector similarity search → Add similar links                    |

---

## 5. Note-taking & Summaries Intents

| User Intent                      | AI Interpretation      | Backend Actions                                   |
| -------------------------------- | ---------------------- | ------------------------------------------------- |
| "Summarize this"                 | Content summarization  | → Summaries using LLM → Store summary as `Note`   |
| "Turn this into study notes"     | Note structuring       | → Convert raw text → bullet notes → Create `Note` |
| "Extract key points"             | Information extraction | → Create structured note → Index embedding        |
| "Connect this note to my course" | Link note to course    | → Update `Note.linkedCourseId`                    |

---

## 6. Progress Tracking Intents

| User Intent          | AI Interpretation      | Backend Actions                                              |
| -------------------- | ---------------------- | ------------------------------------------------------------ |
| "How am I doing?"    | Progress check         | → Analyze goal progress, schedule adherence → Return metrics |
| "Am I on track?"     | Forecast check         | → Run forecast model → Show probability of achieving goals   |
| "Update my progress" | Manual progress update | → Create progress event → Recalculate goal progress          |

---

## 7. Assistant Control Intents

| User Intent                  | AI Interpretation          | Backend Actions                                            |
| ---------------------------- | -------------------------- | ---------------------------------------------------------- |
| "Forget that"                | Clear conversation context | → Reset conversation memory                                |
| "Save this"                  | Save important content     | → Create `Note` or `Resource`                              |
| "Remind me later"            | Set reminder               | → Create `Reminder` entry → Queue notification             |
| "Focus on this course today" | Set active context         | → Prioritize specific course in AI context/recommendations |

---

## 8. Voice Commands (mapped same as text)

Examples:

* "Start a study session" → Begin `Session` timer
* "What should I do next?" → AI computes next immediate task
* "Explain topic X" → AI generates micro-lesson
* "Plan my tomorrow" → Daily schedule generation

---

# 18. Full LLM Prompt Architecture (system prompts + formatting rules)

## Purpose

Provide deterministic, structured, and safe outputs from the LLM so the backend can parse actions reliably (create course, create schedule, recommend resources, etc.). Use a two-layer approach:

1. **System prompt**: global rules (tone, constraints, safety).
2. **Task prompt**: dynamic context + templates for each intent type.

## System Prompt (example)

```
You are Maigie’s AI Assistant. Be concise, helpful, and educational. Always try to ask clarifying questions when user intent is ambiguous. Do NOT make up dates or claim access to a user’s private files. When asked to perform an action (create course, set goal, schedule), return a JSON payload according to the requested `output_schema` and include a short human-readable `summary` field. If unsure, ask one clarifying question. Keep responses under 500 words unless summarizing large content.
```

## Output Formatting Rules

* All action-oriented responses must include a top-level JSON block with exact field `action` and `payload`. Use `JSON_ONLY` wrapper.
* After the JSON block the assistant may include an optional natural-language `explanation` for the user.
* If multiple actions are proposed, include an array `actions`.
* Validate that generated dates are ISO 8601 (`YYYY-MM-DD` or full timestamp).

### Example: JSON_ONLY wrapper

```
JSON_ONLY
{ "action": "create_course", "payload": { ... } }
END_JSON

Human explanation: ...
```

## Example Task Prompt (Create Course)

```
System: <system prompt above>
User: "I want to learn Data Structures in 6 weeks"
Task: create_course
Context: { user_id: U123, timezone: 'Africa/Lagos', active_courses: [...] }
Output Schema: {
  "action": "create_course",
  "payload": {
    "title": "string",
    "description": "string",
    "modules": [{ "title": "string", "duration_days": int, "topics": ["string"] }],
    "start_date": "YYYY-MM-DD",
    "end_date": "YYYY-MM-DD",
    "estimated_hours": float
  }
}

Instructions: produce a JSON_ONLY block strictly following the Output Schema. If any dates are unknown, ask a clarifying question instead of guessing.
```

## Failure Modes & Mitigations

* If LLM returns invalid JSON: run it through a JSON repair pass (small parser + heuristics) then verify schema; if still invalid, ask user a clarifying question.
* For date conflicts: compare generated dates to user-provided deadlines and clamp by availability; present choices to user.
* For hallucinated resources: include `source` metadata and prefer high-precision retrieval (embedding retrieval) rather than pure generation.

---

# 19. Decision Engine Flowchart (AI Intent Routing)

This is the middleware in the backend which converts LLM outputs / user messages into concrete system actions.

## Overview Steps

1. **Receive message** (REST or WebSocket) with auth context and current `active_context` (course/goal selection).
2. **Preprocessing**: sanitize input, detect language, run intent classifier (lightweight model or rules).
3. **Context enrichment**: fetch relevant user data (courses, recent notes, schedule availability) and prepare retrieval buffer.
4. **LLM call**: send system + task prompt to LLM with `output_schema` indicating expected action(s).
5. **Postprocessing**:

   * Validate LLM JSON against schema.
   * If `action` present → route to `Action Dispatcher`.
   * If `clarify` proposed → send clarifying question to user.
6. **Action Dispatcher**: maps `action` to domain service (CourseService, ScheduleService, ResourceService).
7. **Transaction & Event Emission**: perform DB change inside a transactional boundary, then emit domain events.
8. **Response to client**: send `result` back via REST response or WebSocket push.

## Edge Cases

* Ambiguous user intent: capture and ask clarifying question; do not auto-create entities.
* Permission checks: ensure user is allowed to create/modify the target entity.
* Quotas & costs: if action would trigger expensive LLM calls or many creations, throttle and confirm.

---

# 20. Backend Event Architecture (signals when AI creates entities)

Events allow other systems (workers, notif service, analytics) to react to AI-created entities. Use an event bus (Redis streams, Kafka, or simple Postgres pub/sub).

## Event Schema (JSON common envelope)

```
{
  "event_id": "uuid",
  "type": "string",            // e.g. course.created, schedule.created
  "timestamp": "ISO8601",
  "user_id": "uuid",
  "payload": { ... },           // domain payload
  "metadata": { "source": "ai" | "user", "request_id": "uuid" }
}
```

## Core Events

* `course.created` — payload: course object
* `course.updated`
* `goal.created`
* `goal.updated`
* `schedule.created`
* `schedule.updated`
* `resource.recommended` — payload: resource objects + scoring
* `ai.conversation.logged` — payload: conversation id + snapshot

## Example flow

* AI action produces `create_course` JSON → Backend validates → Creates `Course` in DB inside transaction → Emit `course.created` to event bus → Worker `indexing-service` listens and queues embedding job → Another worker `notification-service` may send onboarding tips.

## Event consumers

* Indexing worker (embeddings)
* Notification service (push/email)
* Analytics (track conversion & engagement)
* Sync service (mobile offline sync)

---

# 21. Detailed DB Schema (Prisma)

Below is a pragmatic Prisma schema for the core models. Adjust types to match your DB provider.

```prisma
generator client {
  provider = "prisma-client-js"
}

datasource db {
  provider = "postgresql"
  url      = env("DATABASE_URL")
}

model User {
  id            String    @id @default(cuid())
  email         String    @unique
  name          String?
  passwordHash  String
  timezone      String    @default("Africa/Lagos")
  preferences   Json?
  createdAt     DateTime  @default(now())
  courses       Course[]  @relation("owner_courses")
  enrollments   Enrollment[]
  goals         Goal[]
  notes         Note[]
  schedules     ScheduleBlock[]
}

model Course {
  id          String    @id @default(cuid())
  ownerId     String
  owner       User      @relation(fields: [ownerId], references: [id], name: "owner_courses")
  title       String
  description String?
  metadata    Json?
  modules     Module[]
  enrollments Enrollment[]
  createdAt   DateTime  @default(now())
}

model Module {
  id        String   @id @default(cuid())
  courseId  String
  course    Course   @relation(fields: [courseId], references: [id])
  title     String
  content   String?
  order     Int
}

model Enrollment {
  id        String  @id @default(cuid())
  userId    String
  courseId  String
  role      String  @default("student")
  enrolledAt DateTime @default(now())
  user      User    @relation(fields: [userId], references: [id])
  course    Course  @relation(fields: [courseId], references: [id])
}

model Goal {
  id          String   @id @default(cuid())
  userId      String
  title       String
  description String?
  targetDate  DateTime?
  status      String   @default("active")
  progress    Float    @default(0.0)
  createdAt   DateTime @default(now())
  user        User     @relation(fields: [userId], references: [id])
}

model Task {
  id        String   @id @default(cuid())
  userId    String
  goalId    String?
  courseId  String?
  title     String
  notes     String?
  dueAt     DateTime?
  status    String   @default("todo")
  createdAt DateTime @default(now())
  user      User     @relation(fields: [userId], references: [id])
}

model Note {
  id             String   @id @default(cuid())
  userId         String
  title          String?
  content        String
  linkedCourseId String?
  embeddingsId   String?
  createdAt      DateTime @default(now())
  user           User     @relation(fields: [userId], references: [id])
}

model Resource {
  id          String  @id @default(cuid())
  title       String
  url         String
  type        String
  metadata    Json?
  recommended Boolean @default(false)
  score       Float?
  createdAt   DateTime @default(now())
}

model ScheduleBlock {
  id           String   @id @default(cuid())
  userId       String
  title        String
  startAt      DateTime
  endAt        DateTime
  recurringRule String?
  createdAt    DateTime @default(now())
  user         User     @relation(fields: [userId], references: [id])
}

model AIConversation {
  id         String   @id @default(cuid())
  userId     String
  messages   Json
  metadata   Json?
  createdAt  DateTime @default(now())
  user       User     @relation(fields: [userId], references: [id])
}

model Embedding {
  id         String   @id @default(cuid())
  objectType String
  objectId   String
  vector     Bytes
  createdAt  DateTime @default(now())
}
```

Notes:

* `Embedding.vector` uses `Bytes` — when using Postgres+pgvector you may choose custom type mapping.
* Add indexes on frequently queried fields (e.g. `Resource(url)`, `Note.userId`, `ScheduleBlock.userId`, `Embedding(objectType, objectId)`).
* Add full-text search indices where appropriate.


